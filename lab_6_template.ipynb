{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9e84d7",
   "metadata": {},
   "source": [
    "# Lab 6: APOGEE Stellar Spectra - Bayesian Linear Regression\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Date:** [Date]\n",
    "\n",
    "**Keywords:** Bayesian Linear Regression, Error Propagation, Posterior Distribution, Uncertainty Quantification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we explore Bayesian linear regression using APOGEE stellar spectra data. We'll learn how to:\n",
    "- Properly quantify prediction uncertainties\n",
    "- Understand why naive MLE error estimates fail\n",
    "- Implement Bayesian linear regression with heteroscedastic noise\n",
    "- Make predictions with full uncertainty quantification\n",
    "- Evaluate model calibration using coverage statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53033b6d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure plotting parameters\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['legend.fontsize'] = 20\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8d16e",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Preparing Dataset\n",
    "\n",
    "### Background\n",
    "APOGEE (Apache Point Observatory Galactic Evolution Experiment) provides high-resolution infrared spectra of stars. Each spectrum contains flux measurements at different wavelengths that encode information about stellar properties including temperature.\n",
    "\n",
    "### Task 1a: Load Data and Add Heteroscedastic Noise\n",
    "\n",
    "Load the dataset and add realistic heteroscedastic noise to temperatures:\n",
    "\n",
    "$$\\sigma_{\\text{teff}} = 50 + 150 \\cdot \\frac{T_{\\text{eff}} - T_{\\text{min}}}{T_{\\text{max}} - T_{\\text{min}}}$$\n",
    "\n",
    "This creates uncertainties ranging from 50K (coolest stars) to 200K (hottest stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load 'dataset_apogee_spectra.npz' and extract arrays\n",
    "# TODO: Calculate teff_min, teff_max from teff_array\n",
    "# TODO: Calculate sigma_teff using formula: 50 + 150 * (teff - min)/(max - min)\n",
    "# TODO: Create teff_observed = teff_array + np.random.normal(0, sigma_teff)\n",
    "# TODO: Print summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda1fce",
   "metadata": {},
   "source": [
    "### Task 1b: Construct Design Matrix\n",
    "\n",
    "Create design matrix $\\boldsymbol{\\Phi}$ with:\n",
    "- Each row = one star (N stars)\n",
    "- Each column = one spectral feature (M wavelengths)\n",
    "- Add bias term (column of ones) at the end\n",
    "\n",
    "**Question:** What is the final dimension of your design matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c016ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Phi = np.hstack((spectra, np.ones((len(spectra), 1))))\n",
    "# TODO: Print Phi.shape - should be (N, M+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecc6c5",
   "metadata": {},
   "source": [
    "### Task 1c: Train-Test Split\n",
    "\n",
    "Split data into 80% training and 20% testing, including uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d033f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: n_train = int(0.8 * len(Phi))\n",
    "# TODO: indices = np.random.permutation(len(Phi))\n",
    "# TODO: Split using indices[:n_train] and indices[n_train:]\n",
    "# TODO: Create Phi_train, Phi_test, t_train, t_test, sigma_train, sigma_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe417816",
   "metadata": {},
   "source": [
    "### Task 1d: Visualize Spectra\n",
    "\n",
    "Plot all spectra color-coded by temperature with a colorbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee789408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: colors = plt.cm.plasma((teff_array - teff_min) / (teff_max - teff_min))\n",
    "# TODO: Loop: ax.plot(wavelength, spectra[i], color=colors[i], alpha=0.3)\n",
    "# TODO: Add colorbar with ScalarMappable\n",
    "# TODO: Set labels and show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d8f6c",
   "metadata": {},
   "source": [
    "## Part 2: MLE Error Estimates\n",
    "\n",
    "### Background\n",
    "The maximum likelihood solution for linear regression is:\n",
    "\n",
    "$$\\mathbf{w}_{\\text{ML}} = (\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "### Task 2a: Compute MLE and Naive Uncertainty Estimates\n",
    "\n",
    "Implement the MLE solution and compute RMSE on training and test sets:\n",
    "\n",
    "$$\\sigma_{\\text{naive}} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(t_n - \\mathbf{w}_{\\text{ML}}^T\\boldsymbol{\\phi}_n)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab827a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define fit_naive_mle(Phi_train, t_train, Phi_test, t_test)\n",
    "# TODO: w_ml = np.linalg.inv(Phi_train.T @ Phi_train) @ Phi_train.T @ t_train\n",
    "# TODO: pred_train = Phi_train @ w_ml, pred_test = Phi_test @ w_ml\n",
    "# TODO: sigma_mle = sqrt(mean((t_train - pred_train)^2))\n",
    "# TODO: rmse_test = sqrt(mean((t_test - pred_test)^2))\n",
    "# TODO: Call function and print ratio rmse_test/sigma_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4d60a",
   "metadata": {},
   "source": [
    "### Task 2b: Visualize Residuals\n",
    "\n",
    "Create two plots:\n",
    "1. Histogram of training vs test residuals with Gaussian fits\n",
    "2. Histogram of normalized test residuals compared to N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff08555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create 2 subplots (1,2)\n",
    "# TODO: Left: histogram of residuals_train and residuals_test with Gaussian overlays\n",
    "# TODO: Right: histogram of (residuals_test / sigma_test) vs N(0,1)\n",
    "# TODO: Show that normalized residuals have std >> 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af6e72",
   "metadata": {},
   "source": [
    "**Question:** What do these plots tell us about the naive MLE approach? Why is the normalized residual distribution wider than expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496c0fc",
   "metadata": {},
   "source": [
    "## Part 3: Bayesian Linear Regression\n",
    "\n",
    "### Background\n",
    "Bayesian linear regression computes a posterior distribution:\n",
    "\n",
    "$$p(\\mathbf{w}|\\mathbf{t}, \\boldsymbol{\\Phi}, \\boldsymbol{\\Sigma}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}_N, \\mathbf{S}_N)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{S}_N = (\\mathbf{S}_0^{-1} + \\boldsymbol{\\Phi}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Phi})^{-1}$ (posterior covariance)\n",
    "- $\\mathbf{m}_N = \\mathbf{S}_N(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{t})$ (posterior mean)\n",
    "- $\\mathbf{S}_0 = \\eta^2\\mathbf{I}$ (prior covariance)\n",
    "- $\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_1^2, ..., \\sigma_N^2)$ (measurement covariance)\n",
    "\n",
    "### Task 3a: Compute Posterior Distribution\n",
    "\n",
    "Implement the posterior computation for different prior strengths $\\eta^2$ = [1000, 100000, 10000000]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccf65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define compute_posterior(Phi, t, sigma, eta2)\n",
    "# TODO: S0_inv = np.eye(M) / eta2\n",
    "# TODO: Sigma_inv = np.diag(1 / sigma**2)\n",
    "# TODO: S_N = np.linalg.inv(S0_inv + Phi.T @ Sigma_inv @ Phi)\n",
    "# TODO: m_N = S_N @ (Phi.T @ Sigma_inv @ t)\n",
    "# TODO: Loop eta2_values=[1e3, 1e5, 1e7], compute posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b0e0f",
   "metadata": {},
   "source": [
    "**Questions:** \n",
    "- What do $\\mathbf{m}_N$ and $\\mathbf{S}_N$ represent?\n",
    "- How does this compare to the naive MLE weights $\\mathbf{w}_{\\text{ML}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9942f",
   "metadata": {},
   "source": [
    "### Task 3b: Visualize Spectral Weights\n",
    "\n",
    "Plot the spectral weights with uncertainty bands (±2σ) for each prior strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop posteriors: spectral_weights = m_N[:-1]\n",
    "# TODO: weight_std = np.sqrt(np.diag(S_N[:-1, :-1]))\n",
    "# TODO: plt.plot(wavelength, spectral_weights)\n",
    "# TODO: plt.fill_between(wavelength, weights-2*std, weights+2*std, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb8600",
   "metadata": {},
   "source": [
    "**Question:** Which $\\eta^2$ provides the optimal balance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbd170",
   "metadata": {},
   "source": [
    "## Part 4: Making Bayesian Predictions with Uncertainty\n",
    "\n",
    "### Background\n",
    "The predictive distribution is:\n",
    "\n",
    "$$p(t_*|\\boldsymbol{\\phi}_*, \\mathcal{D}) = \\mathcal{N}(t_*|\\mu_*, \\sigma_*^2)$$\n",
    "\n",
    "with:\n",
    "- Mean: $\\mu_* = \\mathbf{m}_N^T\\boldsymbol{\\phi}_*$\n",
    "- Variance: $\\sigma_*^2 = \\sigma_{\\text{obs},*}^2 + \\boldsymbol{\\phi}_*^T\\mathbf{S}_N\\boldsymbol{\\phi}_* + \\sigma_{\\text{model}}^2$\n",
    "\n",
    "### Task 4a: Implement Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define predict_with_uncertainty(Phi_new, sigma_new, m_N, S_N, sigma_model)\n",
    "# TODO: pred_mean = Phi_new @ m_N\n",
    "# TODO: param_var = np.sum(Phi_new @ S_N * Phi_new, axis=1)\n",
    "# TODO: pred_var = param_var + sigma_new**2 + sigma_model**2\n",
    "# TODO: Return pred_mean, np.sqrt(pred_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17e024",
   "metadata": {},
   "source": [
    "### Task 4b: Visualize Predictions with Uncertainties\n",
    "\n",
    "Plot predicted vs true temperatures with uncertainty bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop posteriors, get pred_mean and pred_std\n",
    "# TODO: plt.scatter(t_test, pred_mean), add y=x line\n",
    "# TODO: rmse = np.sqrt(np.mean((t_test - pred_mean)**2))\n",
    "# TODO: z_scores = (t_test - pred_mean) / pred_std\n",
    "# TODO: coverage = np.mean(np.abs(z_scores) < 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e09e62",
   "metadata": {},
   "source": [
    "### Task 4c: Calculate Coverage\n",
    "\n",
    "Compute the 95% coverage using Z-scores:\n",
    "\n",
    "$$z = \\frac{t_{\\text{true}} - t_{\\text{predicted}}}{\\sigma_{\\text{predicted}}}$$\n",
    "\n",
    "For well-calibrated uncertainties, ~95% of z-scores should have |z| < 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop posteriors\n",
    "# TODO: Get predictions, calculate z_scores and coverage\n",
    "# TODO: Print f\"η²={eta2:.0e}: RMSE={rmse:.1f}K, Coverage={coverage:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb124c",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Which prior gives the best coverage?\n",
    "- What does coverage < 0.95 indicate?\n",
    "- What does coverage > 0.95 indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f2f8f",
   "metadata": {},
   "source": [
    "## Bonus: Visualize Z-score Distribution\n",
    "\n",
    "### Task 4d: Plot Z-scores\n",
    "\n",
    "Compare the z-score distribution to the expected N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: m_N_opt, S_N_opt = posteriors[1]  # eta2=1e5\n",
    "# TODO: Get predictions and z_scores = (t_test - pred_mean) / pred_std\n",
    "# TODO: plt.hist(z_scores), overlay (1/sqrt(2π)) * exp(-x²/2)\n",
    "# TODO: Print mean and std of z_scores (should be ~0 and ~1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ea617",
   "metadata": {},
   "source": [
    "**Question:** Are your uncertainties well-calibrated? How can you tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b791fa",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Naive MLE fails:** Training RMSE severely underestimates prediction uncertainty due to overfitting\n",
    "\n",
    "2. **Bayesian approach:** Properly accounts for parameter uncertainty, measurement noise, and model inadequacy\n",
    "\n",
    "3. **Prior selection:** The prior strength $\\eta^2$ controls the balance between flexibility and regularization\n",
    "\n",
    "4. **Calibration matters:** Well-calibrated uncertainties are essential for scientific inference\n",
    "\n",
    "5. **Uncertainty decomposition:** Understanding different sources of error helps guide improvement efforts\n",
    "\n",
    "**Your findings:**\n",
    "\n",
    "[Write your conclusions here - which prior was optimal? What was the final RMSE and coverage?]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
